{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n", "        import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import streamlit as st\n", "import plotly.graph_objects as go"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from Disparate_impact_streamlit_demo import *"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["st.set_page_config(layout='wide')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_bias(dict,name1,name2,column1,column2,val1,val2):\n", "    bars = []\n", "    bars.append(go.Bar(x=[dict[column1][val1]],\n", "                    y=[column1],\n", "                    marker={'color': 'rgb(250,145,90)'},\n", "                    name=name1,\n", "                    orientation = 'h',\n", "                    width=[0.4,0.4,0.4],\n", "                    textposition='auto'\n", "                    ))\n", "    bars.append(go.Bar(x=[dict[column2][val2]],\n", "                    y=[column2],\n", "                    marker={'color': 'rgb(84, 194, 232)'},\n", "                    name=name2,\n", "                    orientation = 'h',\n", "                    width=[0.4,0.4,0.4],\n", "                    textposition='auto'))\n", "    config = {'displayModeBar': False}\n", "    fig = go.FigureWidget(data=bars)\n", "    fig.layout.height = 350\n", "    fig.layout.width = 350\n", "    return fig"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def select_mitigation_algorithm(dataset_dict):\n", "    st.markdown(\"A variety of algorithms can be used to mitigate bias.\\\n", "                The choice of which to use depends on whether you want \\\n", "                to fix the data (pre-process), the classifier (in-process), \\\n", "                or the predictions (post-process).\")\n", "    st.markdown(\"** Pre-Processing mitigation algorithms**\")\n", "    algo_name = st.radio(\"Select the mitigation Algorithm\",('Disperate Impact Remover', 'Reweighing'))\n", "    if algo_name == 'Disperate Impact Remover':\n", "        st.markdown(\"The algorithm corrects values for \\\n", "            imbalanced selection rates \\\n", "            between unprivileged and privileged groups \\\n", "             at various levels of repair.\")\n", "    else :\n", "        st.markdown(\"The algorithm uses classifier agnostic iterative approach \\\n", "            i.e.first fully train a classifier  \\\n", "            based on uniform weights  \\\n", "             and then appropriately readjust.\")\n", "    dataset_dict['algo_name'] = algo_name\n", "    return dataset_dict\n", "        \n", "    \n", "def bias_check():\n", "    processed_data = pd.read_csv(\"C:\\\\Users\\\\Joshika\\\\Desktop\\\\Navya\\\\CBA\\\\Bias Mitigation\\\\\\\\AIF_final_adult_data_processed.csv\")\n", "    processed_data['sex'] = processed_data['sex'].astype(str) \n", "    processed_data['race'] = processed_data['race'].astype(str)\n", "    columns = processed_data.select_dtypes(include=['category', object]).columns\n", "    st.markdown(\"**Unbiased** is a condition where the data doesn't show any discrimination against particular group of people\")\n", "    lst1 = []\n", "    lst2= []\n", "    lst3= []\n", "    lst4= []\n", "    dict1={}\n", "    dict2={}\n", "    for column in columns:\n", "        priv_df = processed_data[processed_data[column]=='1.0']\n", "        num_of_previleged = priv_df.shape[0]\n", "        unpriv_df = processed_data[processed_data[column]=='0.0']\n", "        num_of_unprevileged = unpriv_df.shape[0]\n", "    \n", "        unprivileged_outcomes = unpriv_df[unpriv_df['Labels']==1.0].shape[0]\n", "        unprivileged_ratio = (unprivileged_outcomes/num_of_unprevileged)*100\n", "        unprivileged_ratio = round(unprivileged_ratio,2)\n", "    \n", "        privileged_outcomes = priv_df[priv_df['Labels']==1.0].shape[0]\n", "        privileged_ratio = (privileged_outcomes/num_of_previleged)*100\n", "        privileged_ratio = round(privileged_ratio,2)\n", "    \n", "        label0 = processed_data[column].value_counts(normalize=True)['0.0']\n", "        label0 = round(label0,2)*100\n", "        label1 = processed_data[column].value_counts(normalize=True)['1.0']\n", "        label1 = round(label1,2)*100\n", "        \n", "        lst1.append(unprivileged_ratio)\n", "        lst2.append(privileged_ratio)\n", "        lst3.append(label0)\n", "        lst4.append(label1)\n", "    dict1 = {'Unprevileged':lst1,'Privileged':lst2}\n", "    dict2 = {'Label0':lst3,'Label1':lst4}\n", "    \n", "    #col1, col2 = st.beta_columns(2)\n", "    col3, col4 = st.beta_columns(2)\n", "    sex_plot = plot_bias(dict1,\"Female\",\"Male\",'Unprevileged','Privileged',0,0)\n", "    race_plot = plot_bias(dict1,\"Black\",\"White\",\"Unprevileged\",\"Privileged\",1,1)\n", "    orig_sex = plot_bias(dict2,\"Female\",\"Male\",\"Label0\",'Label1',0,0)\n", "    orig_race = plot_bias(dict2,\"White\",\"Black\",\"Label0\",'Label1',1,1)\n", "    #col1.header(\"Overall Percentage of Male/Female\")\n", "    #col1.plotly_chart(orig_sex,use_container_width=False, config={'displayModeBar': False})\n", "    #col2.header(\"Overall Percentage of Black/White\")\n", "    #col2.plotly_chart(orig_race,use_container_width=False, config={'displayModeBar': False})\n", "    col3.header(\"Percentage of Bias in Sex Column\")\n", "    col3.plotly_chart(sex_plot,use_container_width=False, config={'displayModeBar': False})\n", "    col4.header(\"Percentage of Bias in Race Column\")\n", "    col4.plotly_chart(race_plot,use_container_width=False, config={'displayModeBar': False})\n", "    st.markdown(\"Analysis Output\")\n", "    st.markdown(\"**Sex** Column shows higher percentage for male which indicates inclination of data towards Male compared to female.\")\n", "    st.markdown(\"**Race** Column shows higher percentage for whites which indicates inclination of data towards whites compared to blacks.\")\n", "    \n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_opt_pre_proc_dataset(dataset_name):\n", "    dataset_info = {\n", "        \"Adult income dataset\" : {\n", "            \"original_data_path\" : r\"C:\\\\Users\\\\Mahesh\\\\Desktop\\\\Hackathon\\\\CBA\\\\AIF_Adult_data_original.csv\",\n", "            \"processed_data_path\" : r\"C:\\\\Users\\\\Joshika\\\\Desktop\\\\Navya\\\\CBA\\\\Bias Mitigation\\\\AIF_final_adult_data_processed.csv\",\n", "            \"description\" : \"The Adult income dataset classifies wheather an america adult will be earning more than $50K based on certain attributes\\\n", "                i.e age, education-num, sex, capital-gain, capital-loss, hours-per-week. Here sex is the protected attribute\",\n", "            \"seed\" : 119\n", "        },\n", "         \"TW Credit Risk Dataset\" : {\n", "            \"original_data_path\" : r\"D:\\CBA_wok_docs\\Work\\Ethical_AI\\Datasets\\credit_card_default_taiwan\\Streamlit_data\\Taiwan_credit_default_data_original.csv\",\n", "            \"processed_data_path\" : r\"D:\\CBA_wok_docs\\Work\\Ethical_AI\\Datasets\\credit_card_default_taiwan\\Streamlit_data\\Taiwan_credit_default_data_processed.csv\",\n", "            \"description\" : \"The TW Credit Risk Dataset classifies wheather an america adult will be earning more than $50K based on certain attributes\\\n", "                i.e age, education-num, sex, capital-gain, capital-loss, hours-per-week\",\n", "            \"seed\" : 11\n", "        }\n", "    }\n", "    \n", "    processed_data = pd.read_csv(dataset_info[dataset_name][\"processed_data_path\"])\n", "    return processed_data   "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_reweight_dataset(dataset_dict):\n", "    if(dataset_dict['dataset_name'] == 'Adult income dataset'):\n", "        if dataset_dict['protected_attribute_name'] == 'sex':\n", "            privileged_groups = [{'sex': 1}]\n", "            unprivileged_groups = [{'sex': 0}]\n", "            dataset_orig = load_preproc_data_adult(['sex'])\n", "        else:\n", "            privileged_groups = [{'race': 1}]\n", "            unprivileged_groups = [{'race': 0}]\n", "            dataset_orig = load_preproc_data_adult(['race'])\n", "    else:\n", "        pass\n", "    return dataset_orig"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_dataset(dataset_dict):\n", "    dataset_dict = {'dataset_name':'Adult income dataset',\n", "                    'algo_name' : 'Reweighing',\n", "                    'protected_attribute_name' : 'sex'}\n", "    if(dataset_dict['algo_name'] == 'Optimized Pre-processing'):\n", "        dataset_orig = get_opt_pre_proc_dataset(dataset_dict['dataset_name'])\n", "    else:\n", "        dataset_orig = get_reweight_dataset(dataset_dict)\n", "    return dataset_orig"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_dataset_UI():\n", "    button_status = False\n", "    dataset_dict = {}\n", "    emp_st= st.empty()\n", "    dataset_name = st.radio(\"Datasets on which analysis can be done\",('Compas(ProPublica recidivism)', 'German credit scoring', 'Adult census income'))\n", "    if(dataset_name == \"Adult census income\"):\n", "        st.write(\"Classifies wheather an american adult will be earning more than $50K/year\")\n", "        data = pd.read_csv(\"C:\\\\Users\\\\Joshika\\\\Desktop\\\\Navya\\\\CBA\\\\Bias Mitigation\\\\AIF_final_adult_data_processed.csv\")\n", "        st.write(data.head())\n", "        \n", "        \n", "    elif(dataset_name == \"Compas(ProPublica recidivism)\"):\n", "        st.write(\"Predict a criminal defendant\u00e2\u20ac\u2122s likelihood of reoffending\")\n", "        \n", "    else:\n", "        st.write(\"Predict an individual's credit risk.\")\n", "    return dataset_dict"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def compare_results(original_value, repaired_value):\n", "    result = \"\"\n", "    if(original_value > repaired_value):\n", "        result = \"decreased\"\n", "    elif(original_value < repaired_value):\n", "        result = \"increased\"\n", "    else:\n", "        result = \"equal\"\n", "    return result"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def key_points(acc_dict, sp_dict, di_dict):\n", "    acc_result = compare_results(acc_dict['Original'], acc_dict['Repaired'])\n", "    sp_result = compare_results(sp_dict['Original'], sp_dict['Repaired'])\n", "    di_result = compare_results(di_dict['Original'], di_dict['Repaired'])\n", "    if(acc_result == 'equal'):\n", "        acc_statement = \"Accuracy remains same for original and repaired data at \"+str(acc_dict['Original'])\n", "    else:\n", "        acc_statement = \"Accuracy \"+acc_result+\" from \"+str(acc_dict['Original'])+\" to \"+str(acc_dict['Repaired'])\n", "    if(sp_result == 'equal'):\n", "        sp_statement = \"Statistical parity remains same for original and repaired data at \"+str(sp_dict['Original'])\n", "    else:\n", "        sp_statement = \"Statistical parity \"+sp_result+\" from \"+str(sp_dict['Original'])+\" to \"+str(sp_dict['Repaired'])\n", "    if(di_result == 'equal'):\n", "        di_statement = \"Disparate impact remains same for original and repaired data at \"+str(di_dict['Original'])\n", "    else:\n", "        di_statement = \"Disparate impact \"+di_result+\" from \"+str(di_dict['Original'])+\" to \"+str(di_dict['Repaired'])\n", "    return acc_statement, sp_statement, di_statement"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_h_bar(data_dict, thrashold = 0, title = \"Plot title\"):\n", "    bars = []\n", "    bars.append(go.Bar(x=[data_dict[\"Original\"]],\n", "                    y=[\"Original\"],\n", "                    marker={'color': 'rgb(250,145,90)'},\n", "                    name = \"Original\",\n", "                    orientation = 'h',\n", "                    width=[0.4,0.4,0.4],\n", "                    ))\n", "    bars.append(go.Bar(x=[data_dict[\"Repaired\"]],\n", "                    y=[\"Repaired\"],\n", "                    marker={'color': 'rgb(84, 194, 232)'},\n", "                    name = \"Repaired\",\n", "                    orientation = 'h',\n", "                    width=[0.4,0.4,0.4]))\n", "    \n", "    fig = go.FigureWidget(data=bars)\n", "    fig.update_layout(shapes=[\n", "        dict(\n", "        type= 'line',\n", "        yref= 'paper', y0= 0, y1= 1,\n", "        xref= 'x', x0= thrashold, x1= thrashold\n", "        )\n", "    ],\n", "    title={\n", "        'text': title,\n", "        'y':0.9,\n", "        'x':0.5,\n", "        'xanchor': 'center',\n", "        'yanchor': 'top'}\n", "    )\n", "    fig.update(layout_showlegend=False)\n", "    fig.layout.height = 350\n", "    fig.layout.width = 350\n", "    return fig"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def compare_page(dataset_name,protected_attribute,repair_value):\n", "    processed_data = get_opt_pre_proc_dataset(dataset_name)\n", "    seed = 119\n", "    Y = processed_data[\"Labels\"]\n", "    X = processed_data.drop([\"Labels\"], axis = 1)\n", "    column_names = list(processed_data.columns)\n", "    X_train, y_train, X_test, y_test = train_test_split_reindexed(X, Y, seed)\n", "    original_accuracy, original_pr_unpriv, original_pr_priv, \\\n", "    original_disparate_impact = logistic_regression_results(X_train, y_train, X_test, y_test,protected_column = protected_attribute)\n", "    \n", "    X_train_updated, y_train_updated, X_test_updated, y_test_updated = remove_disparate_impact_from_data(X_train, y_train,X_test, y_test,column_names,repair_level_per = repair_value,protected_attribute_name = protected_attribute)\n", "    repaired_accuracy, repaired_pr_unpriv, repaired_pr_priv, \\\n", "    repaired_disparate_impact = logistic_regression_results(X_train_updated, \\\n", "                                                            y_train_updated, \\\n", "                                                            X_test_updated, y_test_updated,\n", "                                                            protected_column = protected_attribute)\n", "    original_statistical_parity = original_pr_unpriv - original_pr_priv\n", "    repaired_statistical_parity = repaired_pr_unpriv - repaired_pr_priv\n", "    acc_dict = {'Original' : round(original_accuracy,3)*100, 'Repaired' : round(repaired_accuracy,3)*100}\n", "    di_dict = {'Original' : round(original_disparate_impact,3), 'Repaired' : round(repaired_disparate_impact,3)}\n", "    sp_dict = {'Original' : round(original_statistical_parity,3), 'Repaired' : round(repaired_statistical_parity,3)}\n", "    st.markdown(\"**Visualisation of Bias/Debias**\")\n", "    col1, col2 , col3= st.beta_columns(3)\n", "    \n", "    di_fig = plot_h_bar(di_dict, thrashold = 1,title = \"Disparate Impact\")\n", "    sp_fig = plot_h_bar(sp_dict, thrashold = -0.001,title = \"Statistical parity difference\")\n", "    acc_fig = plot_h_bar(acc_dict, thrashold = 75.2,title = \"Accuracy\")\n", "    \n", "    \n", "    col1.plotly_chart(di_fig,use_container_width=False, config={'displayModeBar': False})\n", "    col2.plotly_chart(sp_fig,use_container_width=False, config={'displayModeBar': False})\n", "    col3.plotly_chart(acc_fig,use_container_width=False, config={'displayModeBar': False})\n", "    \n", "    acc_statement, sp_statement, di_statement = key_points(acc_dict, sp_dict, di_dict)\n", "    \n", "    st.markdown(\"** Results of the Analysis**\")\n", "    st.markdown(acc_statement)\n", "    st.markdown(sp_statement)\n", "    st.markdown(di_statement)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}